{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAFA-6 Training Notebook\n",
        "\n",
        "This notebook trains a multi-head MLP model for protein function prediction using ProtT5 embeddings.\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. Add the \"cafa-6-dataset\" dataset to this notebook\n",
        "2. Enable GPU accelerator (Settings > Accelerator > GPU T4 x2 or P100)\n",
        "3. Add your wandb API key as a Kaggle Secret named `WANDB_API_KEY`\n",
        "4. Run all cells\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q wandb transformers sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup wandb\n",
        "import os\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "try:\n",
        "    user_secrets = UserSecretsClient()\n",
        "    wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "    os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "    print(\"Wandb API key loaded from Kaggle secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not load wandb API key: {e}\")\n",
        "    print(\"You can still train, but wandb logging will be disabled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Login to wandb\n",
        "try:\n",
        "    wandb.login()\n",
        "    USE_WANDB = True\n",
        "    print(\"Logged into wandb successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Wandb login failed: {e}\")\n",
        "    USE_WANDB = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Data paths (Kaggle competition data)\n",
        "    \"train_fasta\": \"/kaggle/input/cafa-6-dataset/Train/train_sequences.fasta\",\n",
        "    \"train_terms\": \"/kaggle/input/cafa-6-dataset/Train/train_terms.tsv\",\n",
        "    \"go_obo\": \"/kaggle/input/cafa-6-dataset/Train/go-basic.obo\",\n",
        "    \n",
        "    # Model\n",
        "    \"embedding_dim\": 1024,  # ProtT5-XL embedding dimension\n",
        "    \"hidden_dims\": [512, 256],\n",
        "    \"dropout\": 0.1,\n",
        "    \n",
        "    # Training\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"epochs\": 100,\n",
        "    \"patience\": 10,\n",
        "    \"val_split\": 0.1,\n",
        "    \"seed\": 42,\n",
        "    \n",
        "    # Embedding generation\n",
        "    \"embed_batch_size\": 4,  # Smaller for Kaggle GPU memory\n",
        "    \"max_seq_length\": 1024,\n",
        "}\n",
        "\n",
        "# Output paths\n",
        "OUTPUT_DIR = Path(\"/kaggle/working\")\n",
        "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Configuration loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(CONFIG[\"seed\"])\n",
        "print(f\"Random seed set to {CONFIG['seed']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_fasta(fasta_path: str) -> dict[str, str]:\n",
        "    \"\"\"Parse a FASTA file and return a dict of protein_id -> sequence.\"\"\"\n",
        "    sequences = {}\n",
        "    current_id = None\n",
        "    current_seq = []\n",
        "    \n",
        "    with open(fasta_path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\">\"):\n",
        "                if current_id is not None:\n",
        "                    sequences[current_id] = \"\".join(current_seq)\n",
        "                current_id = line[1:].split()[0]\n",
        "                current_seq = []\n",
        "            else:\n",
        "                current_seq.append(line)\n",
        "        \n",
        "        if current_id is not None:\n",
        "            sequences[current_id] = \"\".join(current_seq)\n",
        "    \n",
        "    return sequences\n",
        "\n",
        "# Load training sequences\n",
        "train_sequences = parse_fasta(CONFIG[\"train_fasta\"])\n",
        "print(f\"Loaded {len(train_sequences)} training sequences\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_annotations(terms_path: str) -> dict[str, set[str]]:\n",
        "    \"\"\"Load protein annotations from train_terms.tsv.\"\"\"\n",
        "    annotations = {}\n",
        "    \n",
        "    with open(terms_path) as f:\n",
        "        next(f)  # Skip header\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                protein_id = parts[0]\n",
        "                term = parts[1]\n",
        "                if protein_id not in annotations:\n",
        "                    annotations[protein_id] = set()\n",
        "                annotations[protein_id].add(term)\n",
        "    \n",
        "    return annotations\n",
        "\n",
        "# Load annotations\n",
        "train_annotations = load_annotations(CONFIG[\"train_terms\"])\n",
        "print(f\"Loaded annotations for {len(train_annotations)} proteins\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_go_terms_from_obo(obo_path: str) -> dict[str, str]:\n",
        "    \"\"\"Load GO term -> namespace mapping from OBO file.\"\"\"\n",
        "    term_to_namespace = {}\n",
        "    current_term = None\n",
        "    current_namespace = None\n",
        "    \n",
        "    namespace_map = {\n",
        "        \"molecular_function\": \"MF\",\n",
        "        \"biological_process\": \"BP\",\n",
        "        \"cellular_component\": \"CC\",\n",
        "    }\n",
        "    \n",
        "    with open(obo_path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == \"[Term]\":\n",
        "                if current_term and current_namespace:\n",
        "                    term_to_namespace[current_term] = current_namespace\n",
        "                current_term = None\n",
        "                current_namespace = None\n",
        "            elif line.startswith(\"id: GO:\"):\n",
        "                current_term = line[4:]\n",
        "            elif line.startswith(\"namespace:\"):\n",
        "                ns = line.split(\": \")[1]\n",
        "                current_namespace = namespace_map.get(ns)\n",
        "        \n",
        "        if current_term and current_namespace:\n",
        "            term_to_namespace[current_term] = current_namespace\n",
        "    \n",
        "    return term_to_namespace\n",
        "\n",
        "# Load GO term namespaces\n",
        "term_to_ontology = load_go_terms_from_obo(CONFIG[\"go_obo\"])\n",
        "print(f\"Loaded {len(term_to_ontology)} GO terms from ontology\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build term indices per ontology\n",
        "all_terms = set()\n",
        "for terms in train_annotations.values():\n",
        "    all_terms.update(terms)\n",
        "\n",
        "mf_terms = sorted([t for t in all_terms if term_to_ontology.get(t) == \"MF\"])\n",
        "bp_terms = sorted([t for t in all_terms if term_to_ontology.get(t) == \"BP\"])\n",
        "cc_terms = sorted([t for t in all_terms if term_to_ontology.get(t) == \"CC\"])\n",
        "\n",
        "mf_term_to_idx = {t: i for i, t in enumerate(mf_terms)}\n",
        "bp_term_to_idx = {t: i for i, t in enumerate(bp_terms)}\n",
        "cc_term_to_idx = {t: i for i, t in enumerate(cc_terms)}\n",
        "\n",
        "print(f\"Term counts:\")\n",
        "print(f\"  MF: {len(mf_terms)}\")\n",
        "print(f\"  BP: {len(bp_terms)}\")\n",
        "print(f\"  CC: {len(cc_terms)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate ProtT5 Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "\n",
        "# Load ProtT5 model\n",
        "print(\"Loading ProtT5-XL model...\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False)\n",
        "model_t5 = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_t5 = model_t5.to(device)\n",
        "model_t5.eval()\n",
        "\n",
        "print(f\"ProtT5 loaded on {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_embeddings(sequences: dict[str, str], batch_size: int = 4, max_length: int = 1024):\n",
        "    \"\"\"Generate ProtT5 embeddings for all sequences.\"\"\"\n",
        "    protein_ids = list(sequences.keys())\n",
        "    embeddings = []\n",
        "    \n",
        "    # Process in batches\n",
        "    for i in tqdm(range(0, len(protein_ids), batch_size), desc=\"Generating embeddings\"):\n",
        "        batch_ids = protein_ids[i:i + batch_size]\n",
        "        batch_seqs = [sequences[pid][:max_length] for pid in batch_ids]\n",
        "        \n",
        "        # Add spaces between amino acids (ProtT5 format)\n",
        "        batch_seqs = [\" \".join(list(seq)) for seq in batch_seqs]\n",
        "        \n",
        "        # Tokenize\n",
        "        encoded = tokenizer(\n",
        "            batch_seqs,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        ).to(device)\n",
        "        \n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model_t5(**encoded)\n",
        "            # Mean pooling over sequence length\n",
        "            attention_mask = encoded[\"attention_mask\"].unsqueeze(-1)\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "            masked_hidden = hidden_states * attention_mask\n",
        "            mean_embedding = masked_hidden.sum(dim=1) / attention_mask.sum(dim=1)\n",
        "            embeddings.append(mean_embedding.cpu().numpy())\n",
        "    \n",
        "    embeddings = np.vstack(embeddings)\n",
        "    return protein_ids, embeddings\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"\\nGenerating embeddings for training sequences...\")\n",
        "protein_ids, embeddings = generate_embeddings(\n",
        "    train_sequences,\n",
        "    batch_size=CONFIG[\"embed_batch_size\"],\n",
        "    max_length=CONFIG[\"max_seq_length\"],\n",
        ")\n",
        "\n",
        "print(f\"\\nGenerated embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "# Free GPU memory\n",
        "del model_t5\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Dataset and DataLoaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProteinDataset(Dataset):\n",
        "    \"\"\"Dataset for protein embeddings and GO term annotations.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        protein_ids: list[str],\n",
        "        embeddings: np.ndarray,\n",
        "        annotations: dict[str, set[str]],\n",
        "        mf_term_to_idx: dict[str, int],\n",
        "        bp_term_to_idx: dict[str, int],\n",
        "        cc_term_to_idx: dict[str, int],\n",
        "    ):\n",
        "        self.protein_ids = protein_ids\n",
        "        self.embeddings = embeddings\n",
        "        self.annotations = annotations\n",
        "        self.mf_term_to_idx = mf_term_to_idx\n",
        "        self.bp_term_to_idx = bp_term_to_idx\n",
        "        self.cc_term_to_idx = cc_term_to_idx\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.protein_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        protein_id = self.protein_ids[idx]\n",
        "        embedding = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
        "        \n",
        "        # Create target vectors\n",
        "        terms = self.annotations.get(protein_id, set())\n",
        "        \n",
        "        targets_mf = torch.zeros(len(self.mf_term_to_idx), dtype=torch.float32)\n",
        "        targets_bp = torch.zeros(len(self.bp_term_to_idx), dtype=torch.float32)\n",
        "        targets_cc = torch.zeros(len(self.cc_term_to_idx), dtype=torch.float32)\n",
        "        \n",
        "        for term in terms:\n",
        "            if term in self.mf_term_to_idx:\n",
        "                targets_mf[self.mf_term_to_idx[term]] = 1.0\n",
        "            elif term in self.bp_term_to_idx:\n",
        "                targets_bp[self.bp_term_to_idx[term]] = 1.0\n",
        "            elif term in self.cc_term_to_idx:\n",
        "                targets_cc[self.cc_term_to_idx[term]] = 1.0\n",
        "        \n",
        "        return {\n",
        "            \"protein_id\": protein_id,\n",
        "            \"embedding\": embedding,\n",
        "            \"targets_mf\": targets_mf,\n",
        "            \"targets_bp\": targets_bp,\n",
        "            \"targets_cc\": targets_cc,\n",
        "        }\n",
        "\n",
        "# Create full dataset\n",
        "full_dataset = ProteinDataset(\n",
        "    protein_ids=protein_ids,\n",
        "    embeddings=embeddings,\n",
        "    annotations=train_annotations,\n",
        "    mf_term_to_idx=mf_term_to_idx,\n",
        "    bp_term_to_idx=bp_term_to_idx,\n",
        "    cc_term_to_idx=cc_term_to_idx,\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {len(full_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train and validation\n",
        "indices = list(range(len(full_dataset)))\n",
        "train_indices, val_indices = train_test_split(\n",
        "    indices,\n",
        "    test_size=CONFIG[\"val_split\"],\n",
        "    random_state=CONFIG[\"seed\"],\n",
        ")\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "val_dataset = Subset(full_dataset, val_indices)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
        "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadMLP(nn.Module):\n",
        "    \"\"\"Multi-head MLP for multi-label GO term prediction.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        hidden_dims: list[int],\n",
        "        num_mf_terms: int,\n",
        "        num_bp_terms: int,\n",
        "        num_cc_terms: int,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.num_mf_terms = num_mf_terms\n",
        "        self.num_bp_terms = num_bp_terms\n",
        "        self.num_cc_terms = num_cc_terms\n",
        "        self.dropout_rate = dropout\n",
        "        \n",
        "        # Shared backbone\n",
        "        layers = []\n",
        "        in_dim = embedding_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(in_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "            ])\n",
        "            in_dim = hidden_dim\n",
        "        self.backbone = nn.Sequential(*layers)\n",
        "        \n",
        "        # Separate heads for each ontology\n",
        "        self.head_mf = nn.Linear(hidden_dims[-1], num_mf_terms)\n",
        "        self.head_bp = nn.Linear(hidden_dims[-1], num_bp_terms)\n",
        "        self.head_cc = nn.Linear(hidden_dims[-1], num_cc_terms)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        return {\n",
        "            \"logits_mf\": self.head_mf(features),\n",
        "            \"logits_bp\": self.head_bp(features),\n",
        "            \"logits_cc\": self.head_cc(features),\n",
        "        }\n",
        "    \n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"embedding_dim\": self.embedding_dim,\n",
        "            \"hidden_dims\": self.hidden_dims,\n",
        "            \"num_mf_terms\": self.num_mf_terms,\n",
        "            \"num_bp_terms\": self.num_bp_terms,\n",
        "            \"num_cc_terms\": self.num_cc_terms,\n",
        "            \"dropout\": self.dropout_rate,\n",
        "        }\n",
        "\n",
        "# Create model\n",
        "model = MultiHeadMLP(\n",
        "    embedding_dim=CONFIG[\"embedding_dim\"],\n",
        "    hidden_dims=CONFIG[\"hidden_dims\"],\n",
        "    num_mf_terms=len(mf_terms),\n",
        "    num_bp_terms=len(bp_terms),\n",
        "    num_cc_terms=len(cc_terms),\n",
        "    dropout=CONFIG[\"dropout\"],\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {num_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping handler.\"\"\"\n",
        "    \n",
        "    def __init__(self, patience: int = 10):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = float(\"inf\")\n",
        "    \n",
        "    def __call__(self, val_loss: float) -> bool:\n",
        "        if val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter >= self.patience\n",
        "\n",
        "# Setup training\n",
        "optimizer = Adam(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG[\"learning_rate\"],\n",
        "    weight_decay=CONFIG[\"weight_decay\"],\n",
        ")\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "early_stopping = EarlyStopping(patience=CONFIG[\"patience\"])\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# Initialize wandb\n",
        "if USE_WANDB:\n",
        "    wandb.init(\n",
        "        project=\"cafa6\",\n",
        "        config={\n",
        "            **CONFIG,\n",
        "            \"num_mf_terms\": len(mf_terms),\n",
        "            \"num_bp_terms\": len(bp_terms),\n",
        "            \"num_cc_terms\": len(cc_terms),\n",
        "            \"num_train_samples\": len(train_dataset),\n",
        "            \"num_val_samples\": len(val_dataset),\n",
        "            \"model_params\": num_params,\n",
        "        },\n",
        "        reinit=True,\n",
        "    )\n",
        "    wandb.watch(model, log=\"gradients\", log_freq=100)\n",
        "    print(\"Wandb run initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch():\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        embeddings = batch[\"embedding\"].to(device)\n",
        "        targets_mf = batch[\"targets_mf\"].to(device)\n",
        "        targets_bp = batch[\"targets_bp\"].to(device)\n",
        "        targets_cc = batch[\"targets_cc\"].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(embeddings)\n",
        "        \n",
        "        loss_mf = criterion(outputs[\"logits_mf\"], targets_mf)\n",
        "        loss_bp = criterion(outputs[\"logits_bp\"], targets_bp)\n",
        "        loss_cc = criterion(outputs[\"logits_cc\"], targets_cc)\n",
        "        loss = (loss_mf + loss_bp + loss_cc) / 3.0\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    return total_loss / max(num_batches, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate():\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "        embeddings = batch[\"embedding\"].to(device)\n",
        "        targets_mf = batch[\"targets_mf\"].to(device)\n",
        "        targets_bp = batch[\"targets_bp\"].to(device)\n",
        "        targets_cc = batch[\"targets_cc\"].to(device)\n",
        "        \n",
        "        outputs = model(embeddings)\n",
        "        \n",
        "        loss_mf = criterion(outputs[\"logits_mf\"], targets_mf)\n",
        "        loss_bp = criterion(outputs[\"logits_bp\"], targets_bp)\n",
        "        loss_cc = criterion(outputs[\"logits_cc\"], targets_cc)\n",
        "        loss = (loss_mf + loss_bp + loss_cc) / 3.0\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    return total_loss / max(num_batches, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "print(f\"\\nStarting training for {CONFIG['epochs']} epochs...\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "history = {\"train_loss\": [], \"val_loss\": []}\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    train_loss = train_epoch()\n",
        "    val_loss = validate()\n",
        "    \n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    \n",
        "    print(f\"Epoch {epoch + 1}/{CONFIG['epochs']} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "    \n",
        "    # Log to wandb\n",
        "    if USE_WANDB:\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"best_val_loss\": min(best_val_loss, val_loss),\n",
        "        })\n",
        "    \n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"val_loss\": val_loss,\n",
        "            \"model_config\": model.get_config(),\n",
        "        }\n",
        "        torch.save(checkpoint, CHECKPOINT_DIR / \"best_model.pt\")\n",
        "        print(\"  -> Saved best model\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if early_stopping(val_loss):\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
        "        break\n",
        "\n",
        "# Save last model\n",
        "checkpoint = {\n",
        "    \"epoch\": epoch,\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    \"val_loss\": val_loss,\n",
        "    \"model_config\": model.get_config(),\n",
        "}\n",
        "torch.save(checkpoint, CHECKPOINT_DIR / \"last_model.pt\")\n",
        "\n",
        "# Finish wandb run\n",
        "if USE_WANDB:\n",
        "    wandb.finish()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Training complete!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save term indices for inference\n",
        "np.savez(\n",
        "    CHECKPOINT_DIR / \"term_index.npz\",\n",
        "    mf_terms=mf_terms,\n",
        "    bp_terms=bp_terms,\n",
        "    cc_terms=cc_terms,\n",
        ")\n",
        "print(f\"Saved term index to {CHECKPOINT_DIR / 'term_index.npz'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history (local visualization)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\", linewidth=2)\n",
        "plt.plot(history[\"val_loss\"], label=\"Val Loss\", linewidth=2)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training History\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig(OUTPUT_DIR / \"training_history.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nPlot saved to {OUTPUT_DIR / 'training_history.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Checkpoint\n",
        "\n",
        "After training completes, download the checkpoint files from:\n",
        "- `/kaggle/working/checkpoints/best_model.pt`\n",
        "- `/kaggle/working/checkpoints/term_index.npz`\n",
        "\n",
        "You can use these for inference on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List output files\n",
        "print(\"Output files:\")\n",
        "for f in CHECKPOINT_DIR.iterdir():\n",
        "    print(f\"  {f.name} ({f.stat().st_size / 1024 / 1024:.1f} MB)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
